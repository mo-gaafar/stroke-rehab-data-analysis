{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Plan\n",
    "\n",
    "- Download the data\n",
    "    - Place the dataset in data/stroke folder !!! (dont forget to do this)\n",
    "    - Load the data into a pandas dataframe?\n",
    "- Preprocess the signals\n",
    "    - Remove the noise\n",
    "    - Normalize the data\n",
    "- Feature extraction\n",
    "    - Extract features from the signals\n",
    "- Train the model\n",
    "    - Train the model using the extracted features\n",
    "- Evaluate the model\n",
    "    - Evaluate the model using the test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "- Load the data into a pandas dataframe from matlab file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data loadmat\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Load the data\n",
    "data_dict = {\n",
    "    \"Patient_1\" : {\n",
    "        \"Pre\":{\n",
    "            \"Train\": loadmat('data/stroke/P1_pre_training.mat'),\n",
    "            \"Test\": loadmat('data/stroke/P1_pre_test.mat')\n",
    "        },\n",
    "        \"Post\":{\n",
    "            \"Train\": loadmat('data/stroke/P1_post_training.mat'),\n",
    "            \"Test\": loadmat('data/stroke/P1_post_test.mat')\n",
    "        }\n",
    "    },\n",
    "    \"Patient_2\" : {\n",
    "        \"Pre\":{\n",
    "            \"Train\": loadmat('data/stroke/P2_pre_training.mat'),\n",
    "            \"Test\": loadmat('data/stroke/P2_pre_test.mat')\n",
    "        },\n",
    "        \"Post\":{\n",
    "            \"Train\": loadmat('data/stroke/P2_post_training.mat'),\n",
    "            \"Test\": loadmat('data/stroke/P2_post_test.mat')\n",
    "        }\n",
    "    },\n",
    "    \"Patient_3\" : {\n",
    "        \"Pre\":{\n",
    "            \"Train\": loadmat('data/stroke/P3_pre_training.mat'),\n",
    "            \"Test\": loadmat('data/stroke/P3_pre_test.mat')\n",
    "        },\n",
    "        \"Post\":{\n",
    "            \"Train\": loadmat('data/stroke/P3_post_training.mat'),\n",
    "            \"Test\": loadmat('data/stroke/P3_post_test.mat')\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'fs', 'trig', 'y'])\n"
     ]
    }
   ],
   "source": [
    "print(data_dict[\"Patient_1\"][\"Pre\"][\"Train\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def feature(data):\n",
    "    W_list = []\n",
    "    scaler = StandardScaler()\n",
    "    for arl in data:\n",
    "        ar2 = []\n",
    "        w = []\n",
    "        for ar in arl:\n",
    "            normalized_arl = scaler.fit_transform(ar.reshape(-1, 1))\n",
    "            ar2.append(normalized_arl)\n",
    "\n",
    "        ar3 = np.array(ar2)\n",
    "        ar3 = np.squeeze(ar3, axis=-1)\n",
    "        print(ar3.shape)\n",
    "        # Wl, scales = cwt(ar3, 'morlet')\n",
    "        # level='4'\n",
    "        for i in range(len(ar3)):\n",
    "            w.append([])\n",
    "            w[i], scales = pywt.cwt(ar3[i], scales=np.arange(\n",
    "                1, 101), wavelet='morl', sampling_period=250)\n",
    "            w[i] = w[i][3:50, 126:626]\n",
    "\n",
    "        # w[1], scales=pywt.cwt(ar3[1],scales=np.arange(1,100),wavelet='morl',sampling_period=250)\n",
    "\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        Wl_normalized = []\n",
    "        for arr in w:\n",
    "                # arr = np.real(arr)\n",
    "                normalized_arr = scaler.fit_transform(arr)\n",
    "                # print(np.max(normalized_arr))\n",
    "                # print(np.min(normalized_arr))\n",
    "\n",
    "                Wl_normalized.append(normalized_arr)\n",
    "\n",
    "        Wl_normalized = np.transpose(np.array(Wl_normalized), (1, 2, 0))\n",
    "        W_list.append(Wl_normalized)\n",
    "\n",
    "    return W_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
